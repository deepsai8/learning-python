{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is part 2 of the NLP basics\n",
    "\n",
    "#Here we will learn with an example data set\n",
    "\n",
    "# Importing Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Now I am getting angry and I want my damn pho.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Honeslty it didn't taste THAT fresh.)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The potatoes were like rubber and you could te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The fries were great too.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A great touch.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1\n",
       "5     Now I am getting angry and I want my damn pho.      0\n",
       "6              Honeslty it didn't taste THAT fresh.)      0\n",
       "7  The potatoes were like rubber and you could te...      0\n",
       "8                          The fries were great too.      1\n",
       "9                                     A great touch.      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries to clean data\n",
    "import re\n",
    "\n",
    "# Natural Language Tool Kit\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# to remove stopword\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# for Stemming propose\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty array\n",
    "# to append clean text\n",
    "corpus = []\n",
    "\n",
    "# 1000 (reviews) rows to clean\n",
    "for i in range(0, 1000):\n",
    "    \n",
    "    # column : \"Review\", row ith\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    \n",
    "    # convert all cases to lower cases\n",
    "    review = review.lower()\n",
    "    \n",
    "    # split to array(default delimiter is \" \")\n",
    "    review = review.split()\n",
    "    \n",
    "    # creating PorterStemmer object to\n",
    "    # take main stem of each word\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    # loop for stemming each word\n",
    "    # in string array at ith row\n",
    "    review = [ps.stem(word) for word in review\n",
    "              if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # rejoin all string array elements\n",
    "    # to create back into a string\n",
    "    review = ' '.join(review)\n",
    "    \n",
    "    # append each string to create\n",
    "    # array of clean text\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# To extract max 1500 feature.\n",
    "# \"max_features\" is attribute to\n",
    "# experiment with to get better results\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "\n",
    "# X contains corpus (dependent variable)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "# y contains answers if review\n",
    "# is positive or negative\n",
    "y = dataset.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into\n",
    "# the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# experiment with \"test_size\"\n",
    "# to get better results\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=501,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Random Forest Classification\n",
    "# to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# n_estimators can be said as number of\n",
    "# trees, experiment with n_estimators\n",
    "# to get better results\n",
    "model = RandomForestClassifier(n_estimators = 501, criterion = 'entropy')\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Classifier\n",
      "Accuracy:  0.764\n",
      "Confusion Matrix: \n",
      " [[105  20]\n",
      " [ 39  86]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion matrix and scoring\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"RF Classifier\")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bias Classifier\n",
      "Accuracy:  0.704\n",
      "Confusion Matrix: \n",
      " [[ 74  51]\n",
      " [ 23 102]]\n"
     ]
    }
   ],
   "source": [
    "# training a Naive Bayes classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "gnb_predictions = gnb.predict(X_test)\n",
    "\n",
    "# accuracy on X_test\n",
    "accuracy = gnb.score(X_test, y_test)\n",
    "\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, gnb_predictions)\n",
    "\n",
    "print(\"Naive Bias Classifier\")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classifier\n",
      "Accuracy:  0.796\n",
      "Confusion Matrix: \n",
      " [[104  21]\n",
      " [ 30  95]]\n"
     ]
    }
   ],
   "source": [
    "# training a linear SVM classifier\n",
    "from sklearn.svm import SVC\n",
    "svm_model_linear = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions = svm_model_linear.predict(X_test)\n",
    "\n",
    "# model accuracy for X_test\n",
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, svm_predictions)\n",
    "\n",
    "print(\"SVM Classifier\")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTree Classifier\n",
      "Accuracy:  0.608\n",
      "Confusion Matrix: \n",
      " [[119   6]\n",
      " [ 92  33]]\n"
     ]
    }
   ],
   "source": [
    "# training a DescisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree_model = DecisionTreeClassifier(max_depth = 2).fit(X_train, y_train)\n",
    "dtree_predictions = dtree_model.predict(X_test)\n",
    "\n",
    "accuracy = dtree_model.score(X_test, y_test)\n",
    "# creating a confusion matrix\n",
    "cm = confusion_matrix(y_test, dtree_predictions)\n",
    "\n",
    "print(\"DTree Classifier\")\n",
    "print (\"Accuracy: \", accuracy)\n",
    "print(\"Confusion Matrix: \\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It turns out SVM is the best classifier here, given default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
