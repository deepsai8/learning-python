{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we eill discuss about text pre-processing\n",
    "# source: https://www.geeksforgeeks.org/introduction-to-natural-language-processing/\n",
    "# import the necessary libraries\n",
    "import nltk\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the 5 quick brown foxes were trying to jump over the 20 little lazy dogs,    oh! they were so cute, but now they aren't!\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#case change to lower case\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "  \n",
    "input_str = \"The 5 Quick Brown foxes were trying to Jump over the 20 Little Lazy Dogs,    oh! they were so cute, but now they aren't!\"\n",
    "text_lowercase(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The  Quick Brown foxes were trying to Jump over the  Little Lazy Dogs,    oh! they were so cute, but now they aren't!\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result\n",
    "\n",
    "remove_numbers(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The five Quick Brown foxes were trying to Jump over the twenty Little Lazy Dogs, oh! they were so cute, but now they aren't!\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how to convert numbers into words as in 5 to five\n",
    "\n",
    "# import the inflect library\n",
    "import inflect\n",
    "p = inflect.engine()\n",
    "\n",
    "\n",
    "def convert_num(text):\n",
    "    # split string into list of words\n",
    "    temp_str = text.split()\n",
    "    # initialise empty list\n",
    "    new_string = []\n",
    "\n",
    "    for word in temp_str:\n",
    "        # if word is a digit, convert the digit\n",
    "        # to numbers and append into the new_string list\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        # append the word as it is\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    # join the words of new_string to form a string\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str\n",
    "\n",
    "\n",
    "convert_num(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 5 Quick Brown foxes were trying to Jump over the 20 Little Lazy Dogs    oh they were so cute but now they arent'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuation\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "remove_punctuation(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 5 Quick Brown foxes were trying to Jump over the 20 Little Lazy Dogs, oh! they were so cute, but now they aren't!\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove whitespace or blanks from text\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "remove_whitespace(input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words using nltk corpus\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# remove stopwords function\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n",
    "remove_stopwords(example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '5',\n",
       " 'Quick',\n",
       " 'Brown',\n",
       " 'foxes',\n",
       " 'trying',\n",
       " 'Jump',\n",
       " '20',\n",
       " 'Little',\n",
       " 'Lazy',\n",
       " 'Dogs',\n",
       " ',',\n",
       " 'oh',\n",
       " '!',\n",
       " 'cute',\n",
       " ',',\n",
       " \"n't\",\n",
       " '!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# There are mainly three algorithms for stemming:\n",
    "#Porter Stemmer, the Snowball Stemmer and the Lancaster Stemmer. \n",
    "#Porter Stemmer is the most common\n",
    "\n",
    "# stem words in the list of tokenised words\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "stem_words(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '5',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'were',\n",
       " 'tri',\n",
       " 'to',\n",
       " 'jump',\n",
       " 'over',\n",
       " 'the',\n",
       " '20',\n",
       " 'littl',\n",
       " 'lazi',\n",
       " 'dog',\n",
       " ',',\n",
       " 'oh',\n",
       " '!',\n",
       " 'they',\n",
       " 'were',\n",
       " 'so',\n",
       " 'cute',\n",
       " ',',\n",
       " 'but',\n",
       " 'now',\n",
       " 'they',\n",
       " 'are',\n",
       " \"n't\",\n",
       " '!']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'science',\n",
       " 'use',\n",
       " 'scientific',\n",
       " 'methods',\n",
       " 'algorithms',\n",
       " 'and',\n",
       " 'many',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "#similar to Stemming, only difference is lemmatizing converts the word into native language\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# lemmatize string\n",
    "def lemmatize(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    # provide context i.e. part-of-speech\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "text = 'data science uses scientific methods algorithms and many types of processes'\n",
    "lemmatize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " '5',\n",
       " 'Quick',\n",
       " 'Brown',\n",
       " 'fox',\n",
       " 'be',\n",
       " 'try',\n",
       " 'to',\n",
       " 'Jump',\n",
       " 'over',\n",
       " 'the',\n",
       " '20',\n",
       " 'Little',\n",
       " 'Lazy',\n",
       " 'Dogs',\n",
       " ',',\n",
       " 'oh',\n",
       " '!',\n",
       " 'they',\n",
       " 'be',\n",
       " 'so',\n",
       " 'cute',\n",
       " ',',\n",
       " 'but',\n",
       " 'now',\n",
       " 'they',\n",
       " 'be',\n",
       " \"n't\",\n",
       " '!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('gave', 'VBD'),\n",
       " ('me', 'PRP'),\n",
       " ('a', 'DT'),\n",
       " ('scare', 'NN')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part of speech tagging\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# convert text into word_tokens with their tags\n",
    "def pos_tagging(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    return pos_tag(word_tokens)\n",
    "\n",
    "pos_tagging('You just gave me a scare')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_result = pos_tagging(remove_punctuation(convert_num(input_str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to C:\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# To understand the above coding, let's take help from the Penn tagset\n",
    "# download the tagset\n",
    "nltk.download('tagsets')\n",
    "\n",
    "# extract information about the tag\n",
    "nltk.help.upenn_tagset('NN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "  is/VBZ\n",
      "  flying/VBG\n",
      "  in/IN\n",
      "  (NP the/DT sky/NN))\n",
      "(NP the/DT little/JJ yellow/JJ bird/NN)\n",
      "(NP the/DT sky/NN)\n"
     ]
    }
   ],
   "source": [
    "#Chunking\n",
    "#Chunking is the process of extracting phrases from unstructured text and more structure to it. \n",
    "#It is also known as shallow parsing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# define chunking function with text and regular\n",
    "# expression representing grammar as parameter\n",
    "def chunking(text, grammar):\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    # label words with part of speech\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "\n",
    "    # create a chunk parser using grammar\n",
    "    chunkParser = nltk.RegexpParser(grammar)\n",
    "\n",
    "    # test it on the list of word tokens with tagged pos\n",
    "    tree = chunkParser.parse(word_pos)\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        print(subtree)\n",
    "    tree.draw()\n",
    "    \n",
    "sentence = 'the little yellow bird is flying in the sky'\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "chunking(sentence, grammar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  (ORGANIZATION GeeksforGeeks/NNP)\n",
      "  so/RB\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Delhi/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  meetup/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "def ner(text):\n",
    "    # tokenize the text\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    # part of speech tagging of words\n",
    "    word_pos = pos_tag(word_tokens)\n",
    "\n",
    "    # tree of word entities\n",
    "    print(ne_chunk(word_pos))\n",
    "\n",
    "text = 'Bill works for GeeksforGeeks so he went to Delhi for a meetup.'\n",
    "ner(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  5/CD\n",
      "  (PERSON Quick/NNP Brown/NNP)\n",
      "  foxes/NNS\n",
      "  were/VBD\n",
      "  trying/VBG\n",
      "  to/TO\n",
      "  (GPE Jump/NNP)\n",
      "  over/IN\n",
      "  the/DT\n",
      "  20/CD\n",
      "  Little/NNP\n",
      "  Lazy/NNP\n",
      "  Dogs/NNP\n",
      "  ,/,\n",
      "  oh/UH\n",
      "  !/.\n",
      "  they/PRP\n",
      "  were/VBD\n",
      "  so/RB\n",
      "  cute/JJ\n",
      "  ,/,\n",
      "  but/CC\n",
      "  now/RB\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  n't/RB\n",
      "  !/.)\n"
     ]
    }
   ],
   "source": [
    "ner(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
